{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-section",
   "metadata": {},
   "source": [
    "# MCP-Powered Voice Agent with DeepSeek\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This notebook implements a comprehensive voice-enabled AI agent that integrates:\n",
    "- **MCP (Model Context Protocol)**: Structured AI model interactions with context management\n",
    "- **DeepSeek Language Model**: Advanced natural language understanding and generation\n",
    "- **Speech Recognition**: Real-time voice input processing\n",
    "- **Text-to-Speech**: Voice output for AI responses\n",
    "- **Interactive Interface**: User-friendly Jupyter widget controls\n",
    "\n",
    "## üèóÔ∏è System Architecture\n",
    "\n",
    "```\n",
    "Voice Input ‚Üí Speech Recognition ‚Üí Text Processing ‚Üí DeepSeek Model ‚Üí MCP Protocol ‚Üí Response Generation ‚Üí Text-to-Speech ‚Üí Voice Output\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "1. **Audio Processing Pipeline**: Handles microphone input and speaker output\n",
    "2. **MCP Protocol Layer**: Manages conversation context and structured communication\n",
    "3. **DeepSeek Integration**: Provides intelligent language processing\n",
    "4. **Interactive Controls**: Jupyter widgets for user interaction\n",
    "5. **Health Monitoring**: System diagnostics and performance tracking\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Python 3.8+ environment\n",
    "- Microphone access for voice input\n",
    "- Audio output capabilities (speakers/headphones)\n",
    "- Internet connection for model downloads and API access\n",
    "- DeepSeek API key (will be configured in setup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "# 1. Environment Setup and Dependencies\n",
    "\n",
    "This section installs and configures all necessary dependencies for the voice agent.\n",
    "Each dependency serves a specific purpose in our voice processing pipeline.\n",
    "\n",
    "## 1.1 Interactive Widget Setup\n",
    "\n",
    "Jupyter widgets provide the interactive user interface for our voice agent.\n",
    "These widgets enable buttons, progress bars, and real-time status displays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widget-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and configure Jupyter interactive widgets\n",
    "# \n",
    "# ipywidgets: Core library for interactive HTML widgets in Jupyter\n",
    "# - Provides buttons, sliders, text inputs, and output displays\n",
    "# - Essential for creating user-friendly voice agent controls\n",
    "# - Enables real-time status updates and interaction feedback\n",
    "\n",
    "!pip install --upgrade ipywidgets\n",
    "\n",
    "# Enable widget extensions in Jupyter environment\n",
    "# Note: This command may show 'not found' in some environments, but widgets will still function\n",
    "# The extension enables proper widget rendering and interaction handling\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "print(\"‚úÖ Jupyter widgets setup completed\")\n",
    "print(\"üìù Note: 'jupyter-nbextension not found' messages are normal in some environments\")\n",
    "print(\"üéõÔ∏è Interactive controls will be available for voice agent operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml-deps-section",
   "metadata": {},
   "source": [
    "## 1.2 Machine Learning and AI Dependencies\n",
    "\n",
    "These libraries form the core of our AI processing capabilities:\n",
    "\n",
    "- **Transformers**: Hugging Face library providing access to pre-trained language models\n",
    "- **PyTorch**: Deep learning framework that powers most modern NLP models\n",
    "- **Model Integration**: These libraries enable seamless integration with DeepSeek models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core AI and machine learning dependencies\n",
    "\n",
    "# transformers: Hugging Face Transformers library\n",
    "# - Provides access to thousands of pre-trained language models\n",
    "# - Includes automatic tokenizers, model architectures, and utilities\n",
    "# - Supports DeepSeek and other state-of-the-art language models\n",
    "# - Handles model loading, tokenization, and inference pipelines\n",
    "\n",
    "# torch: PyTorch deep learning framework\n",
    "# - Provides tensor operations and neural network building blocks\n",
    "# - Required backend for transformer model computations\n",
    "# - Enables GPU acceleration when available\n",
    "# - Handles automatic differentiation and optimization\n",
    "\n",
    "!pip install transformers torch\n",
    "\n",
    "print(\"‚úÖ Machine Learning dependencies installed successfully\")\n",
    "print(\"ü§ñ Transformer models and PyTorch backend ready\")\n",
    "print(\"üß† DeepSeek integration capabilities enabled\")\n",
    "print(\"‚ö° GPU acceleration available if CUDA is detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audio-deps-section",
   "metadata": {},
   "source": [
    "## 1.3 Audio Processing Dependencies\n",
    "\n",
    "These libraries handle the complete audio pipeline for voice interaction:\n",
    "\n",
    "- **SpeechRecognition**: Converts speech to text using multiple engines\n",
    "- **PyAudio**: Low-level audio I/O for recording and playback\n",
    "- **pyttsx3**: Text-to-speech conversion with multiple voice engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "audio-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install comprehensive audio processing libraries\n",
    "\n",
    "# SpeechRecognition: Advanced speech-to-text library\n",
    "# - Supports multiple recognition engines: Google, Sphinx, Wit.ai, etc.\n",
    "# - Handles microphone input and audio file processing\n",
    "# - Provides noise reduction and audio preprocessing\n",
    "# - Supports multiple languages and locales\n",
    "\n",
    "# pyaudio: Cross-platform audio I/O library\n",
    "# - Provides low-level access to audio hardware\n",
    "# - Enables real-time audio recording from microphone\n",
    "# - Supports audio playback through system speakers\n",
    "# - Required for live audio streaming and processing\n",
    "\n",
    "# pyttsx3: Text-to-speech synthesis library\n",
    "# - Works offline with system TTS engines\n",
    "# - Cross-platform: Windows SAPI, macOS NSSpeechSynthesizer, Linux espeak\n",
    "# - Configurable voice properties: rate, volume, voice selection\n",
    "# - No internet connection required for speech synthesis\n",
    "\n",
    "!pip install SpeechRecognition pyaudio pyttsx3\n",
    "\n",
    "print(\"‚úÖ Audio processing libraries installed successfully\")\n",
    "print(\"üé§ Speech recognition engine ready\")\n",
    "print(\"üîä Text-to-speech synthesis ready\")\n",
    "print(\"üéöÔ∏è Audio I/O capabilities enabled\")\n",
    "print(\"‚ö†Ô∏è  Note: If PyAudio fails, system audio development libraries may be needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-deps-section",
   "metadata": {},
   "source": [
    "## 1.4 API and Communication Dependencies\n",
    "\n",
    "These libraries enable communication with external services and APIs:\n",
    "\n",
    "- **Requests**: HTTP client for API communications\n",
    "- **OpenAI**: Client library compatible with DeepSeek's API format\n",
    "- **aiohttp**: Asynchronous HTTP client for non-blocking operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install API communication and networking libraries\n",
    "\n",
    "# requests: HTTP requests library for API communication\n",
    "# - Simple, elegant HTTP client for Python\n",
    "# - Handles authentication, headers, and request formatting\n",
    "# - Used for synchronous API calls to DeepSeek services\n",
    "# - Includes built-in JSON handling and error management\n",
    "\n",
    "# openai: OpenAI-compatible API client\n",
    "# - Provides standardized interface for AI model APIs\n",
    "# - DeepSeek API is compatible with OpenAI format\n",
    "# - Handles authentication, rate limiting, and error handling\n",
    "# - Supports streaming responses and async operations\n",
    "\n",
    "# aiohttp: Asynchronous HTTP client/server framework\n",
    "# - Enables non-blocking HTTP operations\n",
    "# - Useful for concurrent API calls and streaming\n",
    "# - Improves responsiveness during voice processing\n",
    "# - Supports WebSocket connections for real-time communication\n",
    "\n",
    "!pip install requests openai aiohttp\n",
    "\n",
    "print(\"‚úÖ API communication libraries installed successfully\")\n",
    "print(\"üåê HTTP client capabilities ready\")\n",
    "print(\"üîó DeepSeek API integration enabled\")\n",
    "print(\"‚ö° Asynchronous operations supported\")\n",
    "print(\"üîê Authentication and security features available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "# 2. Library Imports and System Initialization\n",
    "\n",
    "This section imports all required libraries and initializes core system components.\n",
    "Each import group serves a specific functional area of the voice agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORE PYTHON LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import os                    # Operating system interface for environment variables\n",
    "import json                  # JSON data handling for API requests/responses\n",
    "import time                  # Time operations for delays and timestamps\n",
    "import threading             # Multi-threading for concurrent operations\n",
    "import asyncio               # Asynchronous programming support\n",
    "import logging               # Logging system for debugging and monitoring\n",
    "from typing import Dict, List, Optional, Any, Union  # Type hints for better code clarity\n",
    "from datetime import datetime, timedelta             # Date/time operations\n",
    "\n",
    "# ============================================================================\n",
    "# AUDIO PROCESSING LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import speech_recognition as sr  # Speech-to-text conversion engine\n",
    "import pyttsx3                   # Text-to-speech synthesis engine\n",
    "import pyaudio                   # Low-level audio input/output operations\n",
    "\n",
    "# ============================================================================\n",
    "# MACHINE LEARNING AND NLP LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import torch                     # PyTorch deep learning framework\n",
    "from transformers import (\n",
    "    AutoTokenizer,               # Automatic tokenizer selection for models\n",
    "    AutoModelForCausalLM,        # Causal language model architecture (GPT-style)\n",
    "    pipeline,                    # High-level model interface for common tasks\n",
    "    BitsAndBytesConfig          # Quantization configuration for memory efficiency\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# API AND NETWORKING LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import requests                  # HTTP requests for API communication\n",
    "from openai import OpenAI        # OpenAI-compatible client for DeepSeek API\n",
    "import aiohttp                   # Asynchronous HTTP client\n",
    "\n",
    "# ============================================================================\n",
    "# JUPYTER NOTEBOOK LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import ipywidgets as widgets     # Interactive HTML widgets for Jupyter\n",
    "from IPython.display import (\n",
    "    display,                     # Display objects in notebook cells\n",
    "    clear_output,                # Clear cell output programmatically\n",
    "    HTML,                        # Render HTML content\n",
    "    Audio,                       # Audio playback widget\n",
    "    Javascript                   # Execute JavaScript in browser\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Configure comprehensive logging system\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Console output\n",
    "        logging.FileHandler('voice_agent.log', mode='a')  # File logging\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create logger instance for this module\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM COMPATIBILITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"üêç Python version: {os.sys.version}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers available: {hasattr(torch, 'cuda') and torch.cuda.is_available()}\")\n",
    "print(f\"üéÆ CUDA available: {torch.cuda.is_available() if hasattr(torch, 'cuda') else False}\")\n",
    "print(\"üì¶ Core components ready for initialization\")\n",
    "\n",
    "logger.info(\"Voice Agent initialization started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "# 3. Configuration Management System\n",
    "\n",
    "This section defines a comprehensive configuration system that centralizes all settings\n",
    "for the voice agent. This approach makes the system easily configurable and maintainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceAgentConfig:\n",
    "    \"\"\"\n",
    "    Comprehensive configuration class for the Voice Agent system.\n",
    "    \n",
    "    This class centralizes all configuration parameters, making it easy to:\n",
    "    - Adjust system behavior without modifying code throughout the notebook\n",
    "    - Maintain consistent settings across all components\n",
    "    - Enable easy deployment with different configurations\n",
    "    - Provide clear documentation for each parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # API AND MODEL CONFIGURATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    # DeepSeek API Configuration\n",
    "    DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\", \"your-deepseek-api-key-here\")\n",
    "    DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"  # Official DeepSeek API endpoint\n",
    "    \n",
    "    # Model Selection and Parameters\n",
    "    MODEL_NAME = \"deepseek-chat\"         # Primary model for conversation\n",
    "    FALLBACK_MODEL = \"deepseek-coder\"    # Fallback model if primary fails\n",
    "    MAX_TOKENS = 1000                    # Maximum tokens in model response\n",
    "    TEMPERATURE = 0.7                    # Response creativity (0.0 = deterministic, 1.0 = creative)\n",
    "    TOP_P = 0.9                         # Nucleus sampling parameter\n",
    "    FREQUENCY_PENALTY = 0.0             # Penalty for repeated tokens\n",
    "    PRESENCE_PENALTY = 0.0              # Penalty for new topics\n",
    "    \n",
    "    # ========================================================================\n",
    "    # AUDIO PROCESSING CONFIGURATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Audio Input/Output Settings\n",
    "    SAMPLE_RATE = 16000                 # Audio sample rate in Hz (16kHz standard for speech)\n",
    "    CHUNK_SIZE = 1024                   # Audio chunk size for processing (larger = more latency, better quality)\n",
    "    AUDIO_FORMAT = pyaudio.paInt16      # 16-bit integer audio format\n",
    "    CHANNELS = 1                        # Mono audio (1 channel)\n",
    "    RECORD_SECONDS = 5                  # Maximum recording duration per session\n",
    "    \n",
    "    # Speech Recognition Settings\n",
    "    RECOGNITION_TIMEOUT = 5             # Timeout for speech recognition (seconds)\n",
    "    PHRASE_TIMEOUT = 1                  # Pause detection timeout between phrases\n",
    "    ENERGY_THRESHOLD = 300              # Minimum audio energy for speech detection\n",
    "    DYNAMIC_ENERGY_THRESHOLD = True     # Automatically adjust energy threshold\n",
    "    RECOGNITION_LANGUAGE = \"en-US\"      # Primary language for speech recognition\n",
    "    FALLBACK_LANGUAGES = [\"en-GB\", \"en-AU\"]  # Alternative languages to try\n",
    "    \n",
    "    # Text-to-Speech Configuration\n",
    "    TTS_RATE = 200                      # Speech rate in words per minute\n",
    "    TTS_VOLUME = 0.9                    # Speech volume (0.0 to 1.0)\n",
    "    TTS_VOICE_INDEX = 0                 # Voice selection index (system dependent)\n",
    "    TTS_ENGINE = \"default\"              # TTS engine preference\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MCP PROTOCOL CONFIGURATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Model Context Protocol Settings\n",
    "    MCP_VERSION = \"1.0\"                 # MCP protocol version\n",
    "    MCP_TIMEOUT = 30                    # MCP operation timeout (seconds)\n",
    "    CONTEXT_WINDOW_SIZE = 10            # Number of previous interactions to remember\n",
    "    MAX_CONTEXT_TOKENS = 2048           # Maximum tokens for context history\n",
    "    CONTEXT_COMPRESSION_RATIO = 0.7     # Compress context when approaching limit\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SYSTEM PERFORMANCE CONFIGURATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Threading and Concurrency\n",
    "    MAX_WORKER_THREADS = 4              # Maximum concurrent processing threads\n",
    "    ASYNC_TIMEOUT = 10                  # Timeout for async operations\n",
    "    \n",
    "    # Memory Management\n",
    "    MAX_MEMORY_MB = 1024                # Maximum memory usage in MB\n",
    "    GARBAGE_COLLECTION_INTERVAL = 100   # GC interval in operations\n",
    "    \n",
    "    # Caching Configuration\n",
    "    ENABLE_MODEL_CACHE = True           # Cache loaded models\n",
    "    CACHE_EXPIRY_MINUTES = 60           # Cache expiration time\n",
    "    \n",
    "    # ========================================================================\n",
    "    # USER INTERFACE CONFIGURATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Widget Display Settings\n",
    "    WIDGET_WIDTH = \"300px\"              # Default widget width\n",
    "    WIDGET_HEIGHT = \"40px\"              # Default widget height\n",
    "    UPDATE_INTERVAL_MS = 100            # UI update interval in milliseconds\n",
    "    \n",
    "    # Status and Feedback\n",
    "    SHOW_DEBUG_INFO = True              # Display debug information\n",
    "    ENABLE_PROGRESS_BARS = True         # Show progress indicators\n",
    "    VOICE_FEEDBACK = True               # Provide audio feedback for actions\n",
    "    \n",
    "    # ========================================================================\n",
    "    # LOGGING AND MONITORING\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Logging Configuration\n",
    "    LOG_LEVEL = \"INFO\"                  # Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "    LOG_FILE = \"voice_agent.log\"        # Log file name\n",
    "    MAX_LOG_SIZE_MB = 10                # Maximum log file size\n",
    "    LOG_BACKUP_COUNT = 3                # Number of backup log files\n",
    "    \n",
    "    # Performance Monitoring\n",
    "    ENABLE_METRICS = True               # Collect performance metrics\n",
    "    METRICS_INTERVAL = 60               # Metrics collection interval (seconds)\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_config(cls) -> List[str]:\n",
    "        \"\"\"\n",
    "        Validate configuration settings and return list of issues.\n",
    "        \n",
    "        Returns:\n",
    "            List of configuration validation warnings/errors\n",
    "        \"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # API Key Validation\n",
    "        if cls.DEEPSEEK_API_KEY == \"your-deepseek-api-key-here\":\n",
    "            issues.append(\"‚ö†Ô∏è  DeepSeek API key not configured\")\n",
    "        \n",
    "        # Audio Settings Validation\n",
    "        if cls.SAMPLE_RATE not in [8000, 16000, 44100, 48000]:\n",
    "            issues.append(f\"‚ö†Ô∏è  Unusual sample rate: {cls.SAMPLE_RATE}Hz\")\n",
    "        \n",
    "        if cls.TEMPERATURE < 0.0 or cls.TEMPERATURE > 1.0:\n",
    "            issues.append(f\"‚ö†Ô∏è  Temperature should be 0.0-1.0, got {cls.TEMPERATURE}\")\n",
    "        \n",
    "        # Memory Validation\n",
    "        if cls.MAX_MEMORY_MB < 512:\n",
    "            issues.append(f\"‚ö†Ô∏è  Low memory limit: {cls.MAX_MEMORY_MB}MB\")\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    @classmethod\n",
    "    def get_summary(cls) -> str:\n",
    "        \"\"\"\n",
    "        Get a formatted summary of current configuration.\n",
    "        \n",
    "        Returns:\n",
    "            Formatted configuration summary string\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "üìã Voice Agent Configuration Summary\n",
    "{'='*50}\n",
    "ü§ñ Model: {cls.MODEL_NAME} (temp: {cls.TEMPERATURE})\n",
    "üé§ Audio: {cls.SAMPLE_RATE}Hz, {cls.CHANNELS} channel(s)\n",
    "üó£Ô∏è  TTS: {cls.TTS_RATE}WPM, volume {cls.TTS_VOLUME}\n",
    "üß† Context: {cls.CONTEXT_WINDOW_SIZE} interactions, {cls.MAX_CONTEXT_TOKENS} tokens\n",
    "‚ö° Performance: {cls.MAX_WORKER_THREADS} threads, {cls.MAX_MEMORY_MB}MB limit\n",
    "üìä Monitoring: {'Enabled' if cls.ENABLE_METRICS else 'Disabled'}\n",
    "        \"\"\".strip()\n",
    "\n",
    "# Initialize configuration and validate settings\n",
    "config = VoiceAgentConfig()\n",
    "validation_issues = config.validate_config()\n",
    "\n",
    "print(\"‚úÖ Configuration system initialized\")\n",
    "print(config.get_summary())\n",
    "\n",
    "if validation_issues:\n",
    "    print(\"\\n‚ö†Ô∏è  Configuration Issues:\")\n",
    "    for issue in validation_issues:\n",
    "        print(f\"   {issue}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All configuration settings validated successfully\")\n",
    "\n",
    "logger.info(f\"Configuration loaded: {len(validation_issues)} issues found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-section",
   "metadata": {},
   "source": [
    "# 4. MCP (Model Context Protocol) Implementation\n",
    "\n",
    "The Model Context Protocol provides a structured framework for managing AI model interactions.\n",
    "This implementation handles conversation context, session management, and structured communication\n",
    "between the user interface and the AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcp-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPProtocol:\n",
    "    \"\"\"\n",
    "    Model Context Protocol implementation for structured AI interactions.\n",
    "    \n",
    "    The MCP system provides:\n",
    "    - Conversation context management with memory optimization\n",
    "    - Structured request/response handling with validation\n",
    "    - Session state management and persistence\n",
    "    - Error handling and recovery mechanisms\n",
    "    - Performance monitoring and metrics collection\n",
    "    \n",
    "    This protocol ensures consistent communication format between all system\n",
    "    components and maintains conversation coherence across interactions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: VoiceAgentConfig):\n",
    "        \"\"\"\n",
    "        Initialize MCP protocol handler with comprehensive state management.\n",
    "        \n",
    "        Args:\n",
    "            config: Voice agent configuration object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "        # Session Management\n",
    "        self.session_id = f\"session_{int(time.time())}_{os.getpid()}\"\n",
    "        self.session_start_time = time.time()\n",
    "        \n",
    "        # Context and Memory Management\n",
    "        self.context_history: List[Dict[str, Any]] = []  # Complete conversation history\n",
    "        self.compressed_context: List[Dict[str, Any]] = []  # Compressed older context\n",
    "        self.active_context_tokens = 0  # Current context token count\n",
    "        \n",
    "        # Session Metadata\n",
    "        self.metadata = {\n",
    "            \"mcp_version\": config.MCP_VERSION,\n",
    "            \"session_id\": self.session_id,\n",
    "            \"created_at\": time.time(),\n",
    "            \"model_name\": config.MODEL_NAME,\n",
    "            \"language\": config.RECOGNITION_LANGUAGE,\n",
    "            \"client_info\": {\n",
    "                \"platform\": os.name,\n",
    "                \"python_version\": os.sys.version,\n",
    "                \"torch_version\": torch.__version__\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Performance Metrics\n",
    "        self.metrics = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"total_tokens_processed\": 0,\n",
    "            \"average_response_time\": 0.0,\n",
    "            \"context_compressions\": 0\n",
    "        }\n",
    "        \n",
    "        # State Management\n",
    "        self.is_active = True\n",
    "        self.last_activity = time.time()\n",
    "        \n",
    "        logger.info(f\"MCP Protocol initialized - Session: {self.session_id}\")\n",
    "    \n",
    "    def create_request(self, user_input: str, context: Optional[Dict] = None, \n",
    "                      request_type: str = \"chat\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create a structured MCP request with comprehensive metadata.\n",
    "        \n",
    "        This method packages user input into a standardized format that includes:\n",
    "        - Request identification and routing information\n",
    "        - Complete conversation context with token management\n",
    "        - Session state and metadata\n",
    "        - Performance tracking information\n",
    "        \n",
    "        Args:\n",
    "            user_input: User's text input to process\n",
    "            context: Additional context information (optional)\n",
    "            request_type: Type of request (chat, command, query, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            Structured MCP request dictionary ready for model processing\n",
    "        \"\"\"\n",
    "        request_id = f\"req_{self.metrics['total_requests']}_{int(time.time())}\"\n",
    "        request_timestamp = time.time()\n",
    "        \n",
    "        # Prepare conversation context with token management\n",
    "        context_for_request = self._prepare_context_for_request()\n",
    "        \n",
    "        # Build comprehensive request structure\n",
    "        request = {\n",
    "            # Core MCP Headers\n",
    "            \"mcp_version\": self.config.MCP_VERSION,\n",
    "            \"session_id\": self.session_id,\n",
    "            \"request_id\": request_id,\n",
    "            \"timestamp\": request_timestamp,\n",
    "            \"request_type\": request_type,\n",
    "            \n",
    "            # User Input and Context\n",
    "            \"user_input\": user_input,\n",
    "            \"input_length\": len(user_input),\n",
    "            \"additional_context\": context or {},\n",
    "            \n",
    "            # Conversation History\n",
    "            \"conversation_context\": context_for_request,\n",
    "            \"context_token_count\": self.active_context_tokens,\n",
    "            \"context_compression_level\": len(self.compressed_context),\n",
    "            \n",
    "            # Model Configuration\n",
    "            \"model_config\": {\n",
    "                \"model_name\": self.config.MODEL_NAME,\n",
    "                \"max_tokens\": self.config.MAX_TOKENS,\n",
    "                \"temperature\": self.config.TEMPERATURE,\n",
    "                \"top_p\": self.config.TOP_P\n",
    "            },\n",
    "            \n",
    "            # Session Information\n",
    "            \"session_info\": {\n",
    "                \"session_duration\": request_timestamp - self.session_start_time,\n",
    "                \"total_interactions\": len(self.context_history),\n",
    "                \"last_activity\": self.last_activity\n",
    "            },\n",
    "            \n",
    "            # System Metadata\n",
    "            \"system_metadata\": self.metadata.copy()\n",
    "        }\n",
    "        \n",
    "        # Update metrics and state\n",
    "        self.metrics[\"total_requests\"] += 1\n",
    "        self.last_activity = request_timestamp\n",
    "        \n",
    "        logger.info(f\"MCP request created: {request_id} (type: {request_type})\")\n",
    "        logger.debug(f\"Request context tokens: {self.active_context_tokens}\")\n",
    "        \n",
    "        return request\n",
    "    \n",
    "    def process_response(self, response: str, request_id: str, \n",
    "                        processing_time: float = 0.0) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process and structure an MCP response with comprehensive metadata.\n",
    "        \n",
    "        This method handles the AI model's response by:\n",
    "        - Structuring the response in MCP format\n",
    "        - Adding performance and quality metrics\n",
    "        - Updating conversation context and history\n",
    "        - Managing memory and token limits\n",
    "        \n",
    "        Args:\n",
    "            response: Model's response text\n",
    "            request_id: ID of the original request\n",
    "            processing_time: Time taken to generate response (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            Structured MCP response dictionary with metadata\n",
    "        \"\"\"\n",
    "        response_timestamp = time.time()\n",
    "        response_tokens = len(response.split())  # Approximate token count\n",
    "        \n",
    "        # Build comprehensive response structure\n",
    "        response_data = {\n",
    "            # Core MCP Headers\n",
    "            \"mcp_version\": self.config.MCP_VERSION,\n",
    "            \"session_id\": self.session_id,\n",
    "            \"request_id\": request_id,\n",
    "            \"response_id\": f\"resp_{request_id}_{int(response_timestamp)}\",\n",
    "            \"timestamp\": response_timestamp,\n",
    "            \n",
    "            # Response Content\n",
    "            \"response\": response,\n",
    "            \"response_length\": len(response),\n",
    "            \"response_tokens\": response_tokens,\n",
    "            \"status\": \"success\",\n",
    "            \n",
    "            # Performance Metrics\n",
    "            \"performance\": {\n",
    "                \"processing_time\": processing_time,\n",
    "                \"tokens_per_second\": response_tokens / max(processing_time, 0.001),\n",
    "                \"response_quality_score\": self._calculate_response_quality(response)\n",
    "            },\n",
    "            \n",
    "            # Model Information\n",
    "            \"model_info\": {\n",
    "                \"model_name\": self.config.MODEL_NAME,\n",
    "                \"tokens_used\": response_tokens,\n",
    "                \"context_tokens\": self.active_context_tokens\n",
    "            },\n",
    "            \n",
    "            # Context State\n",
    "            \"context_state\": {\n",
    "                \"total_interactions\": len(self.context_history) + 1,\n",
    "                \"context_window_full\": len(self.context_history) >= self.config.CONTEXT_WINDOW_SIZE,\n",
    "                \"compression_needed\": self.active_context_tokens > self.config.MAX_CONTEXT_TOKENS\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add to conversation context\n",
    "        interaction_record = {\n",
    "            \"request_id\": request_id,\n",
    "            \"timestamp\": response_timestamp,\n",
    "            \"user_input\": \"\",  # Will be filled by caller if needed\n",
    "            \"response\": response,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"tokens\": response_tokens\n",
    "        }\n",
    "        \n",
    "        self.context_history.append(interaction_record)\n",
    "        self.active_context_tokens += response_tokens\n",
    "        \n",
    "        # Manage context size and compression\n",
    "        self._manage_context_size()\n",
    "        \n",
    "        # Update performance metrics\n",
    "        self._update_performance_metrics(processing_time, response_tokens, True)\n",
    "        \n",
    "        logger.info(f\"MCP response processed: {request_id}\")\n",
    "        logger.debug(f\"Response tokens: {response_tokens}, Total context: {self.active_context_tokens}\")\n",
    "        \n",
    "        return response_data\n",
    "    \n",
    "    def _prepare_context_for_request(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Prepare conversation context for model request with intelligent truncation.\n",
    "        \n",
    "        Returns:\n",
    "            Optimized context history for model consumption\n",
    "        \"\"\"\n",
    "        # Return recent context within token limits\n",
    "        context_window = self.context_history[-self.config.CONTEXT_WINDOW_SIZE:]\n",
    "        \n",
    "        # Add compressed context summary if available\n",
    "        if self.compressed_context:\n",
    "            context_summary = {\n",
    "                \"type\": \"context_summary\",\n",
    "                \"summary\": f\"Previous {len(self.compressed_context)} interactions compressed\",\n",
    "                \"key_topics\": self._extract_key_topics(self.compressed_context)\n",
    "            }\n",
    "            return [context_summary] + context_window\n",
    "        \n",
    "        return context_window\n",
    "    \n",
    "    def _manage_context_size(self):\n",
    "        \"\"\"\n",
    "        Manage context size through intelligent compression and pruning.\n",
    "        \"\"\"\n",
    "        if self.active_context_tokens > self.config.MAX_CONTEXT_TOKENS:\n",
    "            # Move older interactions to compressed context\n",
    "            compress_count = max(1, len(self.context_history) // 4)\n",
    "            to_compress = self.context_history[:compress_count]\n",
    "            \n",
    "            # Add to compressed context\n",
    "            self.compressed_context.extend(to_compress)\n",
    "            \n",
    "            # Remove from active context\n",
    "            self.context_history = self.context_history[compress_count:]\n",
    "            \n",
    "            # Recalculate token count\n",
    "            self.active_context_tokens = sum(\n",
    "                interaction.get(\"tokens\", 0) for interaction in self.context_history\n",
    "            )\n",
    "            \n",
    "            self.metrics[\"context_compressions\"] += 1\n",
    "            logger.info(f\"Context compressed: {compress_count} interactions moved to compressed storage\")\n",
    "    \n",
    "    def _calculate_response_quality(self, response: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a simple quality score for the response.\n",
    "        \n",
    "        Args:\n",
    "            response: Response text to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Quality score between 0.0 and 1.0\n",
    "        \"\"\"\n",
    "        if not response:\n",
    "            return 0.0\n",
    "        \n",
    "        # Simple heuristics for response quality\n",
    "        score = 0.5  # Base score\n",
    "        \n",
    "        # Length appropriateness (not too short, not too long)\n",
    "        length = len(response)\n",
    "        if 20 <= length <= 500:\n",
    "            score += 0.2\n",
    "        \n",
    "        # Sentence structure (contains periods or question marks)\n",
    "        if any(punct in response for punct in '.?!'):\n",
    "            score += 0.1\n",
    "        \n",
    "        # Coherence (no excessive repetition)\n",
    "        words = response.lower().split()\n",
    "        if len(set(words)) / max(len(words), 1) > 0.5:\n",
    "            score += 0.1\n",
    "        \n",
    "        # Engagement (contains personal pronouns or questions)\n",
    "        if any(word in response.lower() for word in ['you', 'your', 'i', '?']):\n",
    "            score += 0.1\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def _extract_key_topics(self, interactions: List[Dict]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract key topics from compressed interactions.\n",
    "        \n",
    "        Args:\n",
    "            interactions: List of interaction records\n",
    "            \n",
    "        Returns:\n",
    "            List of key topics/themes\n",
    "        \"\"\"\n",
    "        # Simple keyword extraction (can be enhanced with NLP)\n",
    "        all_text = \" \".join(\n",
    "            interaction.get(\"response\", \"\") for interaction in interactions\n",
    "        )\n",
    "        \n",
    "        # Extract meaningful words (excluding common words)\n",
    "        words = all_text.lower().split()\n",
    "        common_words = {\"the\", \"is\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\", \"by\", \"a\", \"an\"}\n",
    "        meaningful_words = [word for word in words if len(word) > 3 and word not in common_words]\n",
    "        \n",
    "        # Return most frequent meaningful words as topics\n",
    "        from collections import Counter\n",
    "        word_counts = Counter(meaningful_words)\n",
    "        return [word for word, count in word_counts.most_common(5)]\n",
    "    \n",
    "    def _update_performance_metrics(self, processing_time: float, \n",
    "                                  tokens: int, success: bool):\n",
    "        \"\"\"\n",
    "        Update performance metrics with latest operation data.\n",
    "        \"\"\"\n",
    "        if success:\n",
    "            self.metrics[\"successful_requests\"] += 1\n",
    "        else:\n",
    "            self.metrics[\"failed_requests\"] += 1\n",
    "        \n",
    "        self.metrics[\"total_tokens_processed\"] += tokens\n",
    "        \n",
    "        # Update average response time\n",
    "        total_successful = self.metrics[\"successful_requests\"]\n",
    "        if total_successful > 0:\n",
    "            current_avg = self.metrics[\"average_response_time\"]\n",
    "            self.metrics[\"average_response_time\"] = (\n",
    "                (current_avg * (total_successful - 1) + processing_time) / total_successful\n",
    "            )\n",
    "    \n",
    "    def get_context_summary(self) -> str:\n",
    "        \"\"\"\n",
    "        Get a comprehensive summary of the current conversation context.\n",
    "        \n",
    "        Returns:\n",
    "            Formatted string summary of conversation context and system state\n",
    "        \"\"\"\n",
    "        session_duration = time.time() - self.session_start_time\n",
    "        \n",
    "        return f\"\"\"\n",
    "üó£Ô∏è  MCP Session Summary\n",
    "{'='*40}\n",
    "Session ID: {self.session_id}\n",
    "Duration: {session_duration:.1f} seconds\n",
    "Total Interactions: {len(self.context_history)}\n",
    "Active Context Tokens: {self.active_context_tokens}\n",
    "Compressed Interactions: {len(self.compressed_context)}\n",
    "\n",
    "üìä Performance Metrics:\n",
    "  ‚Ä¢ Total Requests: {self.metrics['total_requests']}\n",
    "  ‚Ä¢ Success Rate: {self.metrics['successful_requests']}/{self.metrics['total_requests']}\n",
    "  ‚Ä¢ Average Response Time: {self.metrics['average_response_time']:.2f}s\n",
    "  ‚Ä¢ Tokens Processed: {self.metrics['total_tokens_processed']}\n",
    "  ‚Ä¢ Context Compressions: {self.metrics['context_compressions']}\n",
    "        \"\"\".strip()\n",
    "    \n",
    "    def reset_session(self):\n",
    "        \"\"\"\n",
    "        Reset the session state while preserving configuration.\n",
    "        \"\"\"\n",
    "        old_session_id = self.session_id\n",
    "        \n",
    "        # Create new session\n",
    "        self.session_id = f\"session_{int(time.time())}_{os.getpid()}\"\n",
    "        self.session_start_time = time.time()\n",
    "        \n",
    "        # Clear context\n",
    "        self.context_history.clear()\n",
    "        self.compressed_context.clear()\n",
    "        self.active_context_tokens = 0\n",
    "        \n",
    "        # Reset metrics\n",
    "        self.metrics = {\n",
    "            \"total_requests\": 0,\n",
    "            \"successful_requests\": 0,\n",
    "            \"failed_requests\": 0,\n",
    "            \"total_tokens_processed\": 0,\n",
    "            \"average_response_time\": 0.0,\n",
    "            \"context_compressions\": 0\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"MCP session reset: {old_session_id} -> {self.session_id}\")\n",
    "\n",
    "# Initialize MCP Protocol\n",
    "mcp = MCPProtocol(config)\n",
    "\n",
    "print(\"‚úÖ MCP Protocol system initialized\")\n",
    "print(f\"üÜî Session ID: {mcp.session_id}\")\n",
    "print(f\"üìä Context window: {config.CONTEXT_WINDOW_SIZE} interactions\")\n",
    "print(f\"üß† Token limit: {config.MAX_CONTEXT_TOKENS} tokens\")\n",
    "print(\"üîÑ Context compression and management enabled\")\n",
    "\n",
    "logger.info(\"MCP Protocol initialization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deepseek-section",
   "metadata": {},
   "source": [
    "# 5. DeepSeek Model Integration\n",
    "\n",
    "This section implements comprehensive integration with DeepSeek language models,\n",
    "providing both API-based and local model support with advanced features like\n",
    "memory management, context optimization, and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deepseek-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekIntegration:\n",
    "    \"\"\"\n",
    "    Comprehensive DeepSeek model integration with advanced features.\n",
    "    \n",
    "    This class provides:\n",
    "    - API-based and local model support\n",
    "    - Intelligent context management and memory optimization\n",
    "    - Multi-language support and conversation continuity\n",
    "    - Error handling with fallback mechanisms\n",
    "    - Performance monitoring and optimization\n",
    "    - Token usage tracking and cost management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: VoiceAgentConfig, mcp_protocol: MCPProtocol):\n",
    "        \"\"\"\n",
    "        Initialize DeepSeek integration with comprehensive configuration.\n",
    "        \n",
    "        Args:\n",
    "            config: Voice agent configuration\n",
    "            mcp_protocol: MCP protocol handler for structured communication\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.mcp = mcp_protocol\n",
    "        \n",
    "        # API Client Configuration\n",
    "        self.api_client = None\n",
    "        self.api_available = False\n",
    "        \n",
    "        # Local Model Configuration\n",
    "        self.local_model = None\n",
    "        self.local_tokenizer = None\n",
    "        self.local_model_available = False\n",
    "        \n",
    "        # Performance Tracking\n",
    "        self.performance_stats = {\n",
    "            \"total_api_calls\": 0,\n",
    "            \"total_local_calls\": 0,\n",
    "            \"api_failures\": 0,\n",
    "            \"local_failures\": 0,\n",
    "            \"average_api_latency\": 0.0,\n",
    "            \"average_local_latency\": 0.0,\n",
    "            \"total_tokens_consumed\": 0,\n",
    "            \"estimated_cost_usd\": 0.0\n",
    "        }\n",
    "        \n",
    "        # Conversation State\n",
    "        self.conversation_memory = []  # Structured conversation history\n",
    "        self.user_preferences = {}     # Learned user preferences\n",
    "        self.context_keywords = set()  # Important context keywords\n",
    "        \n",
    "        # Initialize available backends\n",
    "        self._initialize_api_client()\n",
    "        self._initialize_local_model()\n",
    "        \n",
    "        logger.info(\"DeepSeek integration initialized\")\n",
    "    \n",
    "    def _initialize_api_client(self):\n",
    "        \"\"\"\n",
    "        Initialize DeepSeek API client with authentication and error handling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.config.DEEPSEEK_API_KEY != \"your-deepseek-api-key-here\":\n",
    "                self.api_client = OpenAI(\n",
    "                    api_key=self.config.DEEPSEEK_API_KEY,\n",
    "                    base_url=self.config.DEEPSEEK_BASE_URL\n",
    "                )\n",
    "                \n",
    "                # Test API connectivity\n",
    "                test_response = self.api_client.chat.completions.create(\n",
    "                    model=self.config.MODEL_NAME,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                    max_tokens=5\n",
    "                )\n",
    "                \n",
    "                self.api_available = True\n",
    "                logger.info(\"DeepSeek API client initialized and tested successfully\")\n",
    "                \n",
    "            else:\n",
    "                logger.warning(\"DeepSeek API key not configured\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize DeepSeek API: {e}\")\n",
    "            self.api_available = False\n",
    "    \n",
    "    def _initialize_local_model(self):\n",
    "        \"\"\"\n",
    "        Initialize local DeepSeek model for offline operation.\n",
    "        \n",
    "        Note: This requires significant computational resources and model downloads.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if we have enough memory for local model\n",
    "            import psutil\n",
    "            available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "            \n",
    "            if available_memory_gb < 8:  # Require at least 8GB for local model\n",
    "                logger.warning(f\"Insufficient memory for local model: {available_memory_gb:.1f}GB available\")\n",
    "                return\n",
    "            \n",
    "            # Try to load a smaller, compatible model for local use\n",
    "            model_name = \"microsoft/DialoGPT-small\"  # Fallback conversational model\n",
    "            \n",
    "            logger.info(f\"Loading local model: {model_name}\")\n",
    "            \n",
    "            # Load tokenizer\n",
    "            self.local_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.local_tokenizer.pad_token = self.local_tokenizer.eos_token\n",
    "            \n",
    "            # Load model with optimization\n",
    "            self.local_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            self.local_model_available = True\n",
    "            logger.info(\"Local model initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize local model: {e}\")\n",
    "            self.local_model_available = False\n",
    "    \n",
    "    def generate_response(self, user_input: str, context: Optional[Dict] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using available DeepSeek models with intelligent fallback.\n",
    "        \n",
    "        This method:\n",
    "        1. Creates structured MCP request\n",
    "        2. Attempts API-based generation first\n",
    "        3. Falls back to local model if API unavailable\n",
    "        4. Manages conversation context and memory\n",
    "        5. Tracks performance and usage metrics\n",
    "        \n",
    "        Args:\n",
    "            user_input: User's input text\n",
    "            context: Additional context information\n",
    "            \n",
    "        Returns:\n",
    "            Generated response text\n",
    "        \"\"\"\n",
    "        if not user_input.strip():\n",
    "            return \"I didn't hear anything. Could you please repeat that?\"\n",
    "        \n",
    "        # Create MCP request\n",
    "        mcp_request = self.mcp.create_request(user_input, context)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Try API-based generation first\n",
    "            if self.api_available:\n",
    "                response = self._generate_api_response(mcp_request)\n",
    "                method_used = \"API\"\n",
    "            elif self.local_model_available:\n",
    "                response = self._generate_local_response(mcp_request)\n",
    "                method_used = \"Local\"\n",
    "            else:\n",
    "                response = self._generate_fallback_response(user_input)\n",
    "                method_used = \"Fallback\"\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Process response through MCP\n",
    "            mcp_response = self.mcp.process_response(\n",
    "                response, mcp_request[\"request_id\"], processing_time\n",
    "            )\n",
    "            \n",
    "            # Update conversation memory\n",
    "            self._update_conversation_memory(user_input, response, method_used)\n",
    "            \n",
    "            # Learn from interaction\n",
    "            self._learn_from_interaction(user_input, response)\n",
    "            \n",
    "            logger.info(f\"Response generated using {method_used} in {processing_time:.2f}s\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {e}\")\n",
    "            self.performance_stats[\"api_failures\" if self.api_available else \"local_failures\"] += 1\n",
    "            return \"I'm having trouble processing that right now. Could you try rephrasing your question?\"\n",
    "    \n",
    "    def _generate_api_response(self, mcp_request: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using DeepSeek API with advanced context management.\n",
    "        \n",
    "        Args:\n",
    "            mcp_request: Structured MCP request\n",
    "            \n",
    "        Returns:\n",
    "            API-generated response text\n",
    "        \"\"\"\n",
    "        # Prepare conversation messages with context\n",
    "        messages = self._prepare_conversation_messages(mcp_request)\n",
    "        \n",
    "        # Make API call with comprehensive parameters\n",
    "        response = self.api_client.chat.completions.create(\n",
    "            model=self.config.MODEL_NAME,\n",
    "            messages=messages,\n",
    "            max_tokens=self.config.MAX_TOKENS,\n",
    "            temperature=self.config.TEMPERATURE,\n",
    "            top_p=self.config.TOP_P,\n",
    "            frequency_penalty=self.config.FREQUENCY_PENALTY,\n",
    "            presence_penalty=self.config.PRESENCE_PENALTY,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        # Extract and process response\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Update performance statistics\n",
    "        self.performance_stats[\"total_api_calls\"] += 1\n",
    "        self.performance_stats[\"total_tokens_consumed\"] += response.usage.total_tokens\n",
    "        self.performance_stats[\"estimated_cost_usd\"] += self._estimate_cost(response.usage.total_tokens)\n",
    "        \n",
    "        return response_text\n",
    "    \n",
    "    def _generate_local_response(self, mcp_request: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using local model with memory optimization.\n",
    "        \n",
    "        Args:\n",
    "            mcp_request: Structured MCP request\n",
    "            \n",
    "        Returns:\n",
    "            Locally-generated response text\n",
    "        \"\"\"\n",
    "        # Prepare input with conversation context\n",
    "        conversation_text = self._prepare_local_context(mcp_request)\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.local_tokenizer.encode(\n",
    "            conversation_text, \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=1024,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Generate response with controlled parameters\n",
    "        with torch.no_grad():\n",
    "            outputs = self.local_model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=min(self.config.MAX_TOKENS, 200),\n",
    "                temperature=self.config.TEMPERATURE,\n",
    "                top_p=self.config.TOP_P,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.local_tokenizer.eos_token_id,\n",
    "                eos_token_id=self.local_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode and clean response\n",
    "        full_response = self.local_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_text = full_response[len(conversation_text):].strip()\n",
    "        \n",
    "        # Clean up response\n",
    "        if not response_text:\n",
    "            response_text = \"I understand. Could you tell me more about that?\"\n",
    "        \n",
    "        # Update performance statistics\n",
    "        self.performance_stats[\"total_local_calls\"] += 1\n",
    "        \n",
    "        return response_text\n",
    "    \n",
    "    def _generate_fallback_response(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate fallback response when no models are available.\n",
    "        \n",
    "        Args:\n",
    "            user_input: User's input text\n",
    "            \n",
    "        Returns:\n",
    "            Rule-based fallback response\n",
    "        \"\"\"\n",
    "        user_lower = user_input.lower()\n",
    "        \n",
    "        # Simple rule-based responses\n",
    "        if any(greeting in user_lower for greeting in [\"hello\", \"hi\", \"hey\", \"good morning\", \"good afternoon\"]):\n",
    "            return \"Hello! I'm here to help. What would you like to talk about?\"\n",
    "        \n",
    "        elif any(question in user_lower for question in [\"how are you\", \"how do you feel\", \"what's up\"]):\n",
    "            return \"I'm doing well, thank you for asking! How can I assist you today?\"\n",
    "        \n",
    "        elif any(goodbye in user_lower for goodbye in [\"bye\", \"goodbye\", \"see you\", \"farewell\"]):\n",
    "            return \"Goodbye! It was nice talking with you. Have a great day!\"\n",
    "        \n",
    "        elif \"?\" in user_input:\n",
    "            return \"That's an interesting question. I'm currently running in limited mode, but I'd be happy to discuss this with you when my full capabilities are available.\"\n",
    "        \n",
    "        else:\n",
    "            return \"I hear you. Could you help me understand what you'd like to know more about?\"\n",
    "    \n",
    "    def _prepare_conversation_messages(self, mcp_request: Dict) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Prepare conversation messages for API with intelligent context management.\n",
    "        \n",
    "        Args:\n",
    "            mcp_request: MCP request with context\n",
    "            \n",
    "        Returns:\n",
    "            List of conversation messages formatted for API\n",
    "        \"\"\"\n",
    "        messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful, knowledgeable, and friendly AI assistant. Provide clear, accurate, and engaging responses. Keep responses conversational and appropriate for voice interaction.\"\n",
    "        }]\n",
    "        \n",
    "        # Add conversation context\n",
    "        context = mcp_request.get(\"conversation_context\", [])\n",
    "        for interaction in context[-5:]:  # Last 5 interactions\n",
    "            if \"user_input\" in interaction and \"response\" in interaction:\n",
    "                messages.append({\"role\": \"user\", \"content\": interaction[\"user_input\"]})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": interaction[\"response\"]})\n",
    "        \n",
    "        # Add current user input\n",
    "        messages.append({\"role\": \"user\", \"content\": mcp_request[\"user_input\"]})\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def _prepare_local_context(self, mcp_request: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Prepare context string for local model generation.\n",
    "        \n",
    "        Args:\n",
    "            mcp_request: MCP request with context\n",
    "            \n",
    "        Returns:\n",
    "            Context string formatted for local model\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        # Add recent conversation\n",
    "        context = mcp_request.get(\"conversation_context\", [])\n",
    "        for interaction in context[-3:]:  # Last 3 interactions for local model\n",
    "            if \"user_input\" in interaction and \"response\" in interaction:\n",
    "                context_parts.append(f\"Human: {interaction['user_input']}\")\n",
    "                context_parts.append(f\"Assistant: {interaction['response']}\")\n",
    "        \n",
    "        # Add current input\n",
    "        context_parts.append(f\"Human: {mcp_request['user_input']}\")\n",
    "        context_parts.append(\"Assistant:\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _update_conversation_memory(self, user_input: str, response: str, method: str):\n",
    "        \"\"\"\n",
    "        Update conversation memory with interaction details.\n",
    "        \n",
    "        Args:\n",
    "            user_input: User's input\n",
    "            response: Generated response\n",
    "            method: Generation method used\n",
    "        \"\"\"\n",
    "        interaction = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"user_input\": user_input,\n",
    "            \"response\": response,\n",
    "            \"method\": method,\n",
    "            \"user_input_length\": len(user_input),\n",
    "            \"response_length\": len(response)\n",
    "        }\n",
    "        \n",
    "        self.conversation_memory.append(interaction)\n",
    "        \n",
    "        # Keep memory manageable\n",
    "        if len(self.conversation_memory) > 100:\n",
    "            self.conversation_memory = self.conversation_memory[-50:]  # Keep last 50\n",
    "    \n",
    "    def _learn_from_interaction(self, user_input: str, response: str):\n",
    "        \"\"\"\n",
    "        Learn user preferences and context from interactions.\n",
    "        \n",
    "        Args:\n",
    "            user_input: User's input\n",
    "            response: Generated response\n",
    "        \"\"\"\n",
    "        # Extract keywords from user input\n",
    "        words = user_input.lower().split()\n",
    "        meaningful_words = [word for word in words if len(word) > 3]\n",
    "        self.context_keywords.update(meaningful_words)\n",
    "        \n",
    "        # Keep keyword set manageable\n",
    "        if len(self.context_keywords) > 200:\n",
    "            # Keep most recent words (simple approach)\n",
    "            self.context_keywords = set(list(self.context_keywords)[-150:])\n",
    "    \n",
    "    def _estimate_cost(self, tokens: int) -> float:\n",
    "        \"\"\"\n",
    "        Estimate API cost based on token usage.\n",
    "        \n",
    "        Args:\n",
    "            tokens: Number of tokens used\n",
    "            \n",
    "        Returns:\n",
    "            Estimated cost in USD\n",
    "        \"\"\"\n",
    "        # Approximate DeepSeek pricing (adjust as needed)\n",
    "        cost_per_1k_tokens = 0.002  # $0.002 per 1K tokens (estimate)\n",
    "        return (tokens / 1000) * cost_per_1k_tokens\n",
    "    \n",
    "    def get_performance_summary(self) -> str:\n",
    "        \"\"\"\n",
    "        Get comprehensive performance summary.\n",
    "        \n",
    "        Returns:\n",
    "            Formatted performance statistics\n",
    "        \"\"\"\n",
    "        stats = self.performance_stats\n",
    "        total_calls = stats[\"total_api_calls\"] + stats[\"total_local_calls\"]\n",
    "        \n",
    "        return f\"\"\"\n",
    "üöÄ DeepSeek Performance Summary\n",
    "{'='*40}\n",
    "API Status: {'‚úÖ Available' if self.api_available else '‚ùå Unavailable'}\n",
    "Local Model: {'‚úÖ Available' if self.local_model_available else '‚ùå Unavailable'}\n",
    "\n",
    "üìä Usage Statistics:\n",
    "  ‚Ä¢ Total Calls: {total_calls}\n",
    "  ‚Ä¢ API Calls: {stats['total_api_calls']}\n",
    "  ‚Ä¢ Local Calls: {stats['total_local_calls']}\n",
    "  ‚Ä¢ API Failures: {stats['api_failures']}\n",
    "  ‚Ä¢ Local Failures: {stats['local_failures']}\n",
    "\n",
    "‚ö° Performance:\n",
    "  ‚Ä¢ Avg API Latency: {stats['average_api_latency']:.2f}s\n",
    "  ‚Ä¢ Avg Local Latency: {stats['average_local_latency']:.2f}s\n",
    "\n",
    "üí∞ Resource Usage:\n",
    "  ‚Ä¢ Tokens Consumed: {stats['total_tokens_consumed']}\n",
    "  ‚Ä¢ Estimated Cost: ${stats['estimated_cost_usd']:.4f}\n",
    "  ‚Ä¢ Conversations: {len(self.conversation_memory)}\n",
    "  ‚Ä¢ Context Keywords: {len(self.context_keywords)}\n",
    "        \"\"\".strip()\n",
    "\n",
    "# Initialize DeepSeek Integration\n",
    "deepseek = DeepSeekIntegration(config, mcp)\n",
    "\n",
    "print(\"‚úÖ DeepSeek integration initialized\")\n",
    "print(f\"üåê API Available: {deepseek.api_available}\")\n",
    "print(f\"üíª Local Model Available: {deepseek.local_model_available}\")\n",
    "print(\"üß† Intelligent context management enabled\")\n",
    "print(\"üìä Performance monitoring active\")\n",
    "\n",
    "logger.info(\"DeepSeek integration initialization completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}